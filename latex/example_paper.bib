
% https://developer.nvidia.com/blog/migrating-nvidia-nsight-tools-nvvp-nvprof


@misc{nsight_systems,
    title = {Nsight Systems User Guide},
    author = {NVIDIA},
    url = {https://docs.nvidia.com/nsight-systems/UserGuide/index.html}
}

@misc{nsight_compute,
    title = {Nsight Compute User Guide},
    author = {NVIDIA},
    url = {https://docs.nvidia.com/nsight-compute/NsightCompute/index.html}
}


@misc{apex,
    title = {NVIDIA Apex (A PyTorch Extension)},
    author = {NVIDIA},
    year = {2018},
    url = {https://nvidia.github.io/apex/l}
}

@misc{amp,
    title = {Automatic Mixed Precision Package - torch.cuda.amp},
    author = {PyTorch},
    year = {2019},
    url = {https://pytorch.org/docs/stable/amp.html}
}




@inproceedings{gavel,
  author    = {Deepak Narayanan and
               Keshav Santhanam and
               Fiodar Kazhamiaka and
               Amar Phanishayee and
               Matei Zaharia},
  title     = {Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning
               Workloads},
  booktitle = {14th {USENIX} Symposium on Operating Systems Design and Implementation,
               {OSDI} 2020, Virtual Event, November 4-6, 2020},
  pages     = {481--498},
  publisher = {{USENIX} Association},
  year      = {2020},
  url       = {https://www.usenix.org/conference/osdi20/presentation/narayanan-deepak},
  timestamp = {Tue, 02 Feb 2021 08:06:00 +0100},
  biburl    = {https://dblp.org/rec/conf/osdi/NarayananSKPZ20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gandiva,
  author    = {Wencong Xiao and
               Romil Bhardwaj and
               Ramachandran Ramjee and
               Muthian Sivathanu and
               Nipun Kwatra and
               Zhenhua Han and
               Pratyush Patel and
               Xuan Peng and
               Hanyu Zhao and
               Quanlu Zhang and
               Fan Yang and
               Lidong Zhou},
  editor    = {Andrea C. Arpaci{-}Dusseau and
               Geoff Voelker},
  title     = {Gandiva: Introspective Cluster Scheduling for Deep Learning},
  booktitle = {13th {USENIX} Symposium on Operating Systems Design and Implementation,
               {OSDI} 2018, Carlsbad, CA, USA, October 8-10, 2018},
  pages     = {595--610},
  publisher = {{USENIX} Association},
  year      = {2018},
  url       = {https://www.usenix.org/conference/osdi18/presentation/xiao},
  timestamp = {Tue, 02 Feb 2021 08:06:02 +0100},
  biburl    = {https://dblp.org/rec/conf/osdi/XiaoBRSKHPPZZYZ18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% @inproceedings{tiresias,    
%     title={Tiresias: A {GPU} Cluster Manager for Distributed Deep Learning}, ISBN={9781931971492}, 
%     url={https://www.usenix.org/conference/nsdi19/presentation/gu}, 
%     author={Gu, Juncheng and Chowdhury, Mosharaf and Shin, Kang G. and Zhu, Yibo and Jeon, Myeongjae and Qian, Junjie and Liu, Hongqiang and Guo, Chuanxiong}, year={2019}, 
%     pages={485–500} 
% }

% @inproceedings{themis,
%     title={Themis: Fair and Efficient {GPU} Cluster Scheduling},
%     ISBN={9781939133137}, url={https://www.usenix.org/conference/nsdi20/presentation/mahajan},
%     author={Mahajan, Kshiteej and Balasubramanian, Arjun and Singhvi, Arjun and Venkataraman, Shivaram and Akella, Aditya and Phanishayee, Amar and Chawla, Shuchi},
%     year={2020},
%     pages={289–304}
% }

@inproceedings{resnet,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {770--778},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.90},
  doi       = {10.1109/CVPR.2016.90},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/HeZRS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% @misc{vgg,
%       title={Very Deep Convolutional Networks for Large-Scale Image Recognition}, 
%       author={Karen Simonyan and Andrew Zisserman},
%       year={2015},
%       eprint={1409.1556},
%       archivePrefix={arXiv},
%       primaryClass={cs.CV}
% }
% @misc{densenet,
%       title={Densely Connected Convolutional Networks}, 
%       author={Gao Huang and Zhuang Liu and Laurens van der Maaten and Kilian Q. Weinberger},
%       year={2018},
%       eprint={1608.06993},
%       archivePrefix={arXiv},
%       primaryClass={cs.CV}
% }
% @misc{bert,
%       title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
%       author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
%       year={2019},
%       eprint={1810.04805},
%       archivePrefix={arXiv},
%       primaryClass={cs.CL}
% }

@article{lstm,
  author    = {Sepp Hochreiter and
               J{\"{u}}rgen Schmidhuber},
  title     = {Long Short-Term Memory},
  journal   = {Neural Comput.},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  year      = {1997},
  url       = {https://doi.org/10.1162/neco.1997.9.8.1735},
  doi       = {10.1162/neco.1997.9.8.1735},
  timestamp = {Tue, 01 Sep 2020 13:12:40 +0200},
  biburl    = {https://dblp.org/rec/journals/neco/HochreiterS97.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{nvml,
	title={NVIDIA Management Library (NVML)},
	url={https://developer.nvidia.com/nvidia-management-library-nvml},
	journal={NVIDIA Developer},
	year={2021},
	month={Jan}
}

@misc{nvidia-smi,
	title={NVIDIA System Management Interface},
	url={https://developer.nvidia.com/nvidia-system-management-interface},
	journal={NVIDIA Developer},
	year={2021},
	month={Jan}
}

@misc{mig,
	title = {{NVIDIA} {Multi}-{Instance} {GPU} ({MIG})},
	url = {https://www.nvidia.com/en-us/technologies/multi-instance-gpu/},
	abstract = {Expand GPU Access to More Users and Flexibility for Every Workload. {\textless}br/{\textgreater}},
	language = {en-us},
	urldate = {2021-05-12},
	journal = {NVIDIA},
	year={2020},
}

@article{recommendation,
  author    = {Abdallah Moussawi},
  title     = {Towards Large Scale Training Of Autoencoders For Collaborative Filtering},
  journal   = {CoRR},
  volume    = {abs/1809.00999},
  year      = {2018},
  url       = {http://arxiv.org/abs/1809.00999},
  archivePrefix = {arXiv},
  eprint    = {1809.00999},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1809-00999.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mixed_precision,
  author    = {Paulius Micikevicius and
               Sharan Narang and
               Jonah Alben and
               Gregory F. Diamos and
               Erich Elsen and
               David Garc{\'{\i}}a and
               Boris Ginsburg and
               Michael Houston and
               Oleksii Kuchaiev and
               Ganesh Venkatesh and
               Hao Wu},
  title     = {Mixed Precision Training},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=r1gs9JgRZ},
  timestamp = {Thu, 25 Jul 2019 14:26:02 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/MicikeviciusNAD18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@misc{cublas,
	title = {{cuBLAS}},
	url = {https://developer.nvidia.com/cublas},
	abstract = {Basic Linear Algebra on NVIDIA GPUs DOWNLOAD DOCUMENTATION SAMPLES SUPPORT FEEDBACK The cuBLAS Library provides a GPU-accelerated implementation of the basic linear algebra subroutines (BLAS). cuBLAS accelerates AI and HPC applications with drop-in industry standard BLAS APIs highly optimized for NVIDIA GPUs. The cuBLAS library contains extensions for batched operations, execution across multiple GPUs, and mixed and low precision execution. Using cuBLAS, applications automatically benefit from regular performance improvements and new GPU architectures.},
	language = {en},
	urldate = {2021-05-17},
	journal = {NVIDIA Developer},
	month = jul,
	year = {2013},
}

@misc{v100_architecture,
	title = {{NVIDIA} {Tesla} {V100} {GPU} {Architecture}},
	url = {https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf},
	publisher = {NVIDIA},
	month = aug,
	year = {2017},
}


@inproceedings{tpu,
  author    = {Norman P. Jouppi and
               Cliff Young and
               Nishant Patil and
               David A. Patterson and
               Gaurav Agrawal and
               Raminder Bajwa and
               Sarah Bates and
               Suresh Bhatia and
               Nan Boden and
               Al Borchers and
               Rick Boyle and
               Pierre{-}luc Cantin and
               Clifford Chao and
               Chris Clark and
               Jeremy Coriell and
               Mike Daley and
               Matt Dau and
               Jeffrey Dean and
               Ben Gelb and
               Tara Vazir Ghaemmaghami and
               Rajendra Gottipati and
               William Gulland and
               Robert Hagmann and
               C. Richard Ho and
               Doug Hogberg and
               John Hu and
               Robert Hundt and
               Dan Hurt and
               Julian Ibarz and
               Aaron Jaffey and
               Alek Jaworski and
               Alexander Kaplan and
               Harshit Khaitan and
               Daniel Killebrew and
               Andy Koch and
               Naveen Kumar and
               Steve Lacy and
               James Laudon and
               James Law and
               Diemthu Le and
               Chris Leary and
               Zhuyuan Liu and
               Kyle Lucke and
               Alan Lundin and
               Gordon MacKean and
               Adriana Maggiore and
               Maire Mahony and
               Kieran Miller and
               Rahul Nagarajan and
               Ravi Narayanaswami and
               Ray Ni and
               Kathy Nix and
               Thomas Norrie and
               Mark Omernick and
               Narayana Penukonda and
               Andy Phelps and
               Jonathan Ross and
               Matt Ross and
               Amir Salek and
               Emad Samadiani and
               Chris Severn and
               Gregory Sizikov and
               Matthew Snelham and
               Jed Souter and
               Dan Steinberg and
               Andy Swing and
               Mercedes Tan and
               Gregory Thorson and
               Bo Tian and
               Horia Toma and
               Erick Tuttle and
               Vijay Vasudevan and
               Richard Walter and
               Walter Wang and
               Eric Wilcox and
               Doe Hyun Yoon},
  title     = {In-Datacenter Performance Analysis of a Tensor Processing Unit},
  booktitle = {Proceedings of the 44th Annual International Symposium on Computer
               Architecture, {ISCA} 2017, Toronto, ON, Canada, June 24-28, 2017},
  pages     = {1--12},
  publisher = {{ACM}},
  year      = {2017},
  url       = {https://doi.org/10.1145/3079856.3080246},
  doi       = {10.1145/3079856.3080246},
  timestamp = {Tue, 06 Nov 2018 11:07:01 +0100},
  biburl    = {https://dblp.org/rec/conf/isca/JouppiYPPABBBBB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{nvprof,
	title = {Nvidia visual profiler},
	url = {https://developer.nvidia.com/nvidia-visual-profiler},
	abstract = {The NVIDIA Visual Profiler is a cross-platform performance profiling tool that delivers developers vital feedback for optimizing CUDA C/C++ applications. First introduced in 2008, Visual Profiler supports all 350 million+ CUDA capable NVIDIA GPUs shipped since 2006 on Linux, Mac OS X, and Windows. The NVIDIA Visual Profiler is available as part of the CUDA Toolkit. Note that NVIDIA® CUDA Toolkit 11.0 (and later) no longer supports development or running applications on macOS.},
	language = {en},
	urldate = {2021-05-17},
	journal = {NVIDIA Developer},
	month = oct,
	year = {2012},
}

@inproceedings{wavelet,
	title = {Wavelet: {Efficient} {DNN} {Training} with {Tick}-{Tock} {Scheduling}},
	url = {https://mlsys.org/Conferences/2021/ScheduleMultitrack?event=1586},
	abstract = {MLSys Website},
	urldate = {2021-05-17},
	booktitle = {Proceedings of {Machine} {Learning} and {Systems} 3 pre-proceedings},
	author = {Wang, Guanhua and Wang, Kehan and Jiang, Kenan and Li, Xiangjun and Stoica, Ion},
	month = apr,
	year = {2021},
}


@inproceedings{salus,
 author = {Yu, Peifeng and Chowdhury, Mosharaf},
 booktitle = {Proceedings of Machine Learning and Systems 2},
 editor = {I. Dhillon and D. Papailiopoulos and V. Sze},
 pages = {98--111},
 title = {Fine-Grained GPU Sharing Primitives for Deep Learning Applications},
 url = {https://proceedings.mlsys.org/paper/2020/file/f7177163c833dff4b38fc8d2872f1ec6-Paper.pdf},
 volume = {2},
 year = {2020}
}

@misc{mps,
	title = {Multi-{Process} {Service}},
	url = {https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf},
	publisher = {NVIDIA},
	month = jun,
	year = {2020},
}

@misc{mps_tutorial,
	title = {Improving {GPU} {Utilization} with {Multi}-{Process} {Service} ({MPS})},
	url = {https://on-demand.gputechconf.com/gtc/2015/presentation/S5584-Priyanka-Sah.pdf},
	author = {Sah, Priyanka},
	year = {2015},
}


@misc{occupancy,
	title = {Achieved occupancy},
	url = {https://docs.nvidia.com/gameworks/content/developertools/desktop/analysis/report/cudaexperiments/kernellevel/achievedoccupancy.htm},
	urldate = {2021-05-25},
	journal = {NVIDIA Developer},
	year = {2015},
}
