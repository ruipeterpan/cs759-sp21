,name,tensor_util,single_util,double_util,time_%,time_ms,calls,avg_ms
94,volta_sgemm_64x64_nt,0,6,0,14.869603,3.4851300000000003,59387,0.058685
115,"void at::native::vectorized_elementwise_kernel<int=4, at::native::AddFunctor<float>, at::detail::Array<char*, int=3>>(int, float, at::native::AddFunctor<float>)",0,1,0,9.155075,2.145762,581002,0.003693
70,volta_sgemm_128x32_nt,0,7,0,5.73097,1.343222,15629,0.085944
91,volta_sgemm_64x64_nn,0,6,0,4.401147,1.031539,21900,0.047102
73,"void cudnn::winograd_nonfused::winogradForwardFilter4x4<float, __half>(cudnn::winograd_nonfused::WinogradFilterParams<float, __half>)",0,1,0,4.256374,0.997607,71913,0.013872
110,"void cudnn::winograd_nonfused::winogradWgradOutput4x4<float, __half>(cudnn::winograd_nonfused::WinogradWgradOutputParams<float, __half>)",0,1,1,3.7710209999999997,0.88385,43760,0.020197
118,"void cudnn::winograd_nonfused::winogradForwardOutput4x4<float, __half>(cudnn::winograd_nonfused::WinogradOutputParams<float, __half>)",0,3,0,3.744057,0.8775299999999999,71913,0.012202
29,"void cudnn::winograd_nonfused::winogradForwardData4x4<float, __half>(cudnn::winograd_nonfused::WinogradDataParams<float, __half>)",0,2,0,3.408474,0.798876,71913,0.011108
54,volta_fp16_s884cudnn_fp16_128x128_ldg8_splitK_relu_f2f_exp_small_nhwc_tn_v1,3,1,0,3.343614,0.783675,12506,0.062663
83,"void nchwToNhwcKernel<__half, __half, float, bool=1, bool=0, cudnnKernelDataType_t=0>(int, int, int, int, __half const *, __half*, float, float)",0,3,0,2.685866,0.629512,74513,0.008448
84,"void at::native::vectorized_elementwise_kernel<int=4, at::native::MulScalarFunctor<float, float>, at::detail::Array<char*, int=2>>(int, float, float)",0,1,0,2.634764,0.6175350000000001,199876,0.003089
30,_ZN2at6native29vectorized_elementwise_kernelILi4EZNS0_21threshold_kernel_implIN3c104HalfEEEvRNS_14TensorIteratorET_S7_EUlS4_S4_E_NS_6detail5ArrayIPcLi3EEEEEviT0_T1_,0,1,0,2.451816,0.574655,107593,0.005340999999999999
55,_ZN2at6native27unrolled_elementwise_kernelIZZZNS0_21copy_device_to_deviceERNS_14TensorIteratorEbENKUlvE0_clEvENKUlvE6_clEvEUlfE_NS_6detail5ArrayIPcLi2EEE23TrivialOffsetCalculatorILi1EjESC_NS0_6memory12LoadWithCastILi1EEENSD_13StoreWithCastEEEviT_T0_T1_T2_T3_T4_,0,1,0,2.337654,0.547898,71954,0.007614
3,_ZN2at6native27unrolled_elementwise_kernelIZZZNS0_21copy_device_to_deviceERNS_14TensorIteratorEbENKUlvE0_clEvENKUlvE18_clEvEUlN3c104HalfEE_NS_6detail5ArrayIPcLi2EEE23TrivialOffsetCalculatorILi1EjESE_NS0_6memory12LoadWithCastILi1EEENSF_13StoreWithCastEEEviT_T0_T1_T2_T3_T4_,0,1,0,2.292928,0.537415,75101,0.007155
119,_Z25multi_tensor_apply_kernelI18TensorListMetadataILi3EE12AxpbyFunctorIfffEJffiEEviPViT_T0_DpT1_,0,0,0,2.286785,0.535976,6248,0.085783
62,"void cudnn::winograd_nonfused::winogradWgradData4x4<float, __half>(cudnn::winograd_nonfused::WinogradDataParams<float, __half>)",0,2,0,2.127912,0.49873900000000004,43760,0.011397
103,"void cudnn::winograd_nonfused::winogradWgradDelta4x4<float, __half>(cudnn::winograd_nonfused::WinogradDeltaParams<float, __half>)",0,2,0,1.830077,0.428932,43760,0.009801
78,"void at::native::vectorized_elementwise_kernel<int=4, at::native::FillFunctor<float>, at::detail::Array<char*, int=1>>(int, float, at::native::FillFunctor<float>)",0,1,0,1.7816490000000003,0.41758199999999995,196813,0.002121
49,"void cudnn::bn_bw_1C11_singleread_fp16<int=512, int=1, int=2, int=0>(float, float, float, float, cudnnTensorStruct, __half2 const *, cudnn::bn_bw_1C11_singleread_fp16<int=512, int=1, int=2, int=0>, __half2 const , cudnn::bn_bw_1C11_singleread_fp16<int=512, int=1, int=2, int=0>, cudnnTensorStruct*, float const *, float*, float const *, float const , float const , float, cudnn::reduced_divisor, int, float*, cudnn::bnBwPersistentState*, int, float, float, float, int, float, cudnnStatus_t*, bool)",0,2,0,1.754202,0.411149,46875,0.008771
92,"void implicit_convolve_sgemm<__half, __half, int=128, int=5, int=5, int=3, int=3, int=3, int=1, bool=0, bool=1, bool=1>(int, int, int, __half const *, int, __half*, __half const *, kernel_conv_params, __int64, int, float, float, int, __half const *, __half const *, bool, int, int)",0,5,0,1.7263130000000002,0.40461199999999997,6268,0.064552
87,"void implicit_convolve_sgemm<__half, __half, int=1024, int=5, int=5, int=3, int=3, int=3, int=1, bool=0, bool=1, bool=1>(int, int, int, __half const *, int, __half*, __half const *, kernel_conv_params, __int64, int, float, float, int, __half const *, __half const *, bool, int, int)",0,6,0,1.629268,0.381867,3136,0.121768
75,"void cudnn::bn_fw_tr_1C11_singleread_fp16<int=512, int=1, int=2, int=0>(cudnnTensorStruct, __half2 const *, cudnn::bn_fw_tr_1C11_singleread_fp16<int=512, int=1, int=2, int=0>, cudnnTensorStruct*, float const *, float const , float, float, float*, float const *, float const *, float const *, float, float, cudnn::reduced_divisor, int, float*, cudnn::bnFwPersistentState*, int, float, float, float, int, float, float, cudnnStatus_t*, bool)",0,2,0,1.461473,0.342539,46875,0.007306999999999999
52,volta_sgemm_32x128_nn,0,8,0,1.408323,0.33008200000000004,9380,0.035189
57,volta_sgemm_32x128_nt,0,8,0,1.356784,0.318002,9377,0.033913
6,"void cudnn::detail::dgrad_alg1_engine<__half, int=128, int=6, int=7, int=3, int=3, int=5, bool=0, bool=1>(int, int, int, __half const *, int, __half const , int, cudnn::detail::dgrad_alg1_engine<__half, int=128, int=6, int=7, int=3, int=3, int=5, bool=0, bool=1>*, kernel_grad_params, __int64, int, float, int)",0,5,0,1.350267,0.316475,3128,0.101174
98,"void at::native::vectorized_elementwise_kernel<int=4, at::native::AddFunctor<c10::Half>, at::detail::Array<char*, int=3>>(int, c10::Half, at::native::AddFunctor<c10::Half>)",0,1,0,1.255109,0.294172,50632,0.005809
13,volta_s884cudnn_fp16_64x64_sliced1x4_ldg8_wgrad_idx_exp_interior_nhwc_nt_v1,2,1,0,1.252186,0.293487,9379,0.031291
97,volta_fp16_s884cudnn_fp16_128x128_ldg8_splitK_dgrad_f2f_exp_small_nhwc_tt_v1,3,1,0,1.179837,0.27653,6254,0.044216000000000005
113,"void nchwToFoldedNhwcKernel<__half, __half, float, bool=1, cudnnKernelDataType_t=0>(int, int, int, int, __half const *, __half*, int, int, int, int, int, int, int, int, int, int, float, float, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divisor)",0,3,0,0.922502,0.216216,25008,0.008645
14,"void cudnn::bn_bw_1C11_singleread_fp16<int=512, int=1, int=2, int=14>(float, float, float, float, cudnnTensorStruct, __half2 const *, cudnn::bn_bw_1C11_singleread_fp16<int=512, int=1, int=2, int=14>, __half2 const , cudnn::bn_bw_1C11_singleread_fp16<int=512, int=1, int=2, int=14>, cudnnTensorStruct*, float const *, float*, float const *, float const , float const , float, cudnn::reduced_divisor, int, float*, cudnn::bnBwPersistentState*, int, float, float, float, int, float, cudnnStatus_t*, bool)",0,2,0,0.9152040000000001,0.21450500000000003,15625,0.013728
114,volta_s884cudnn_fp16_128x64_sliced1x4_ldg8_wgrad_idx_exp_interior_nhwc_nt_v1,2,1,0,0.816574,0.191388,6252,0.030612
12,volta_fp16_s884cudnn_fp16_128x128_ldg8_dgrad_f2f_exp_small_nhwc2nchw_tt_v1,3,1,0,0.701699,0.164464,6252,0.026305000000000002
7,"void implicit_convolve_sgemm<__half, __half, int=128, int=6, int=7, int=3, int=3, int=5, int=1, bool=0, bool=1, bool=1>(int, int, int, __half const *, int, __half*, __half const *, kernel_conv_params, __int64, int, float, float, int, __half const *, __half const *, bool, int, int)",0,4,0,0.691923,0.162173,3140,0.051647000000000005
120,[CUDA memset],0,0,0,0.6346310000000001,0.14874400000000002,97448,0.001526
82,"void cudnn::bn_fw_tr_1C11_singleread_fp16<int=512, int=1, int=2, int=10>(cudnnTensorStruct, __half2 const *, cudnn::bn_fw_tr_1C11_singleread_fp16<int=512, int=1, int=2, int=10>, cudnnTensorStruct*, float const *, float const , float, float, float*, float const *, float const *, float const *, float, float, cudnn::reduced_divisor, int, float*, cudnn::bnFwPersistentState*, int, float, float, float, int, float, float, cudnnStatus_t*, bool)",0,2,0,0.5820529999999999,0.13642100000000001,15625,0.00873
28,cask_cudnn::computeOffsetsKernel(cask_cudnn::ComputeOffsetsParams),0,1,0,0.554148,0.129881,32788,0.003961
0,"void implicit_convolve_sgemm<__half, __half, int=1024, int=6, int=7, int=3, int=3, int=5, int=1, bool=0, bool=1, bool=1>(int, int, int, __half const *, int, __half*, __half const *, kernel_conv_params, __int64, int, float, float, int, __half const *, __half const *, bool, int, int)",0,3,0,0.473846,0.11105999999999999,3132,0.035459
53,volta_fp16_s884cudnn_fp16_256x64_ldg8_dgrad_f2f_exp_small_nhwc2nchw_tt_v1,4,1,0,0.449578,0.10537200000000001,3127,0.033697000000000005
24,"void at::native::vectorized_elementwise_kernel<int=4, at::native::BUnaryFunctor<at::native::AddFunctor<long>>, at::detail::Array<char*, int=2>>(int, long, at::native::AddFunctor<long>)",0,1,0,0.408377,0.095715,62500,0.001531
100,"void nhwcToNchwKernel<float, __half, float, bool=1, bool=0, cudnnKernelDataType_t=0>(int, int, int, int, float const *, __half*, float, float)",0,3,0,0.402223,0.094273,9381,0.010048999999999999
121,"void cudnn::bn_fw_inf_1C11_kernel_NCHW<__half, float, bool=1, int=1>(float, cudnn::bn_fw_inf_1C11_kernel_NCHW<__half, float, bool=1, int=1>, cudnnTensorStruct, __half const *, float, cudnnTensorStruct*, float, cudnn::bn_fw_inf_1C11_kernel_NCHW<__half, float, bool=1, int=1> const *, cudnn::bn_fw_inf_1C11_kernel_NCHW<__half, float, bool=1, int=1> const , cudnn::bn_fw_inf_1C11_kernel_NCHW<__half, float, bool=1, int=1> const , cudnn::bn_fw_inf_1C11_kernel_NCHW<__half, float, bool=1, int=1> const , cudnn::bn_fw_inf_1C11_kernel_NCHW<__half, float, bool=1, int=1>)",0,0,0,0.401919,0.094201,1580,0.059621
116,volta_fp16_s884cudnn_fp16_256x64_ldg8_relu_f2f_exp_small_nhwc2nchw_tn_v1,5,1,0,0.390954,0.091632,708,0.129423
122,[CUDA memcpy HtoD],0,0,0,0.368104,0.086276,6537,0.013198
17,volta_fp16_s884cudnn_fp16_128x128_ldg8_relu_f2f_exp_interior_nhwc2nchw_tn_v1,3,1,0,0.341217,0.079974,3127,0.025575
48,"void nchwToNhwcKernel<float, __half, float, bool=1, bool=0, cudnnKernelDataType_t=0>(int, int, int, int, float const *, __half*, float, float)",0,5,0,0.336796,0.078938,9381,0.008414
2,volta_s884cudnn_fp16_128x128_ldg8_wgrad_idx_exp_interior_nhwc_nt_v1,5,1,0,0.329262,0.077172,3127,0.024679
108,"void foldedNchwToNchwKernel<__half, __half, float, bool=1, cudnnKernelDataType_t=0>(int, int, int, int, int, int, int, __half const *, __half*, int, int, int, int, int, int, int, int, int, int, int, float, float, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divisor)",0,5,0,0.25556100000000004,0.059898,9380,0.006385
39,cask_cudnn::computeWgradOffsetsKernel(cask_cudnn::ComputeWgradOffsetsParams),0,1,0,0.191828,0.04496,18759,0.002396
16,"void nhwcToNchwKernel<__half, __half, float, bool=1, bool=0, cudnnKernelDataType_t=0>(int, int, int, int, __half const *, __half*, float, float)",0,2,0,0.174985,0.041013,12508,0.003278
123,volta_fp16_s884cudnn_fp16_256x64_ldg8_splitK_relu_f2f_exp_small_nhwc2nchw_tn_v1,0,0,0,0.160252,0.03756,235,0.159829
105,"void foldedNhwcToNchwKernel<__half, __half, float, bool=1, cudnnKernelDataType_t=0>(int, int, int, int, int, int, int, __half const *, __half*, int, int, int, int, int, int, int, int, int, int, float, float, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divisor)",0,4,0,0.159558,0.037397,6252,0.005981
86,volta_fp16_s884cudnn_fp16_256x128_ldg8_relu_f2f_exp_large_nhwc2nchw_tn_v1,6,1,0,0.148923,0.034904000000000004,236,0.1479
26,"void gemmSN_TN_kernel_64addr<float, int=128, int=16, int=2, int=4, int=8, int=9, bool=0, cublasGemvTensorStridedBatched<__half const >, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<__half const , cublasGemvTensorStridedBatched<__half const >, float>)",0,1,0,0.132129,0.030968,3126,0.009906
21,cask_cudnn::computeBOffsetsKernel(cask_cudnn::ComputeBOffsetsParams),0,1,0,0.129184,0.030277999999999996,15634,0.001936
80,"void cudnn::ops::scalePackedTensor_kernel<__half, float>(cudnnTensor4dStruct, __half*, float)",0,1,0,0.106448,0.024949000000000002,3156,0.007905
20,volta_fp16_sgemm_fp16_128x32_nt,0,3,0,0.105244,0.024666999999999998,3125,0.007893
88,"void at::native::reduce_kernel<int=512, int=1, at::native::ReduceOp<c10::Half, at::native::MeanOps<float, float>, unsigned int, c10::Half, int=4>>(c10::Half)",0,2,0,0.093967,0.022024000000000002,3204,0.006873000000000001
124,volta_gcgemm_64x64_nt,0,0,0,0.083807,0.019643,17,1.155456
71,"void at::native::unrolled_elementwise_kernel<at::native::MulScalarFunctor<c10::Half, float>, at::detail::Array<char*, int=2>, OffsetCalculator<int=1, unsigned int>, OffsetCalculator<int=1, unsigned int>, at::native::memory::LoadWithoutCast, at::native::memory::StoreWithoutCast>(int, c10::Half, float, at::native::MulScalarFunctor<c10::Half, float>, char*, int=2, at::detail::Array<char*, int=2>)",0,2,0,0.0822,0.019266,3125,0.0061649999999999995
90,_ZN2at6native13reduce_kernelILi256ELi2ENS0_8ReduceOpIN3c104HalfENS0_14func_wrapper_tIS4_ZNS0_11sum_functorIS4_fS4_EclERNS_14TensorIteratorEEUlffE_EEjS4_Li4EEEEEvT1_,0,1,0,0.07079500000000001,0.016593,3125,0.005309
85,_ZN2at6native27unrolled_elementwise_kernelIZZZNS0_21copy_device_to_deviceERNS_14TensorIteratorEbENKUlvE0_clEvENKUlvE18_clEvEUlN3c104HalfEE_NS_6detail5ArrayIPcLi2EEE16OffsetCalculatorILi1EjESE_NS0_6memory15LoadWithoutCastENSF_16StoreWithoutCastEEEviT_T0_T1_T2_T3_T4_,0,1,0,0.067735,0.015875999999999998,3204,0.004954
76,volta_fp16_s884cudnn_fp16_128x128_ldg8_relu_f2f_exp_large_nhwc2nchw_tn_v1,7,1,0,0.063592,0.014905000000000002,159,0.09374
50,volta_gcgemm_64x32_nt,0,8,0,0.058544000000000006,0.013721,27,0.5082
109,"void gemmSN_NN_kernel<float, int=256, int=4, int=2, int=8, int=4, int=4, bool=0, cublasGemvTensorStridedBatched<__half const >, cublasGemvTensorStridedBatched<__half>>(cublasGemmSmallNParams<__half const , cublasGemvTensorStridedBatched<__half const >, float>)",0,1,0,0.054773,0.012837999999999999,3125,0.004108
19,"void cunn_ClassNLLCriterion_updateOutput_kernel<float, float>(float*, float*, float*, long*, float*, int, int, int, int, long)",0,1,0,0.054275,0.012721,3204,0.0039700000000000004
117,"void implicit_convolve_sgemm<__half, __half, int=512, int=6, int=8, int=3, int=3, int=5, int=1, bool=0, bool=1, bool=1>(int, int, int, __half const *, int, __half*, __half const *, kernel_conv_params, __int64, int, float, float, int, __half const *, __half const *, bool, int, int)",0,7,0,0.046145,0.010816,84,0.12875599999999998
66,"void cunn_ClassNLLCriterion_updateGradInput_kernel<float>(float*, float*, long*, float*, float*, int, int, int, int, long)",0,1,0,0.035455,0.00831,3125,0.002659
1,"void fft2d_r2c_32x32<__half, bool=0, unsigned int=1, bool=0>(float2*, __half const *, int, int, int, int, int, int, int, int, int, cudnn::reduced_divisor, bool, int2, int, int)",0,5,0,0.034459,0.008076,28,0.28844200000000003
93,"void _GLOBAL__N__54_tmpxft_000035b1_00000000_12_SoftMax_compute_80_cpp1_ii_a3310042::softmax_warp_forward<float, float, float, int=4, bool=1>(float*, float const *, int, int, int)",0,1,0,0.033677,0.007893,3204,0.002463
65,"void cudnn::detail::dgrad_engine<__half, int=128, int=6, int=7, int=3, int=3, int=5, bool=0>(int, int, int, __half const *, int, __half const , int, cudnn::detail::dgrad_engine<__half, int=128, int=6, int=7, int=3, int=3, int=5, bool=0>*, kernel_grad_params, __int64, int, __int64, int, float, int, int, int)",0,3,0,0.027835000000000002,0.006523999999999999,14,0.465996
10,"void fft2d_r2c_32x32<__half, bool=0, unsigned int=1, bool=1>(float2*, __half const *, int, int, int, int, int, int, int, int, int, cudnn::reduced_divisor, bool, int2, int, int)",0,5,0,0.027692,0.006490000000000001,16,0.405648
11,volta_scudnn_winograd_128x128_ldg1_ldg4_relu_tile148t_nt_v1,0,8,0,0.027049,0.00634,14,0.45284399999999997
8,volta_fp16_scudnn_fp16_128x32_relu_small_nn_v1,0,4,0,0.026999000000000002,0.006328,83,0.07624
60,volta_cgemm_32x64_tn,0,7,0,0.02655,0.006222999999999999,7,0.888957
32,"void _GLOBAL__N__54_tmpxft_000035b1_00000000_12_SoftMax_compute_80_cpp1_ii_a3310042::softmax_warp_backward<float, float, float, int=4, bool=1>(float*, float const *, float const , int, int, int)",0,1,0,0.025079,0.005878,3125,0.0018800000000000002
4,volta_fp16_scudnn_fp16_128x64_relu_interior_nn_v1,0,6,0,0.023196,0.005437,83,0.065501
74,volta_fp16_scudnn_fp16_128x64_relu_small_nn_v1,0,8,0,0.021705000000000002,0.0050869999999999995,10,0.508729
125,[CUDA memcpy DtoH],0,0,0,0.019665000000000002,0.004609,3315,0.00139
63,"void at::native::vectorized_elementwise_kernel<int=4, at::native::FillFunctor<int>, at::detail::Array<char*, int=1>>(int, int, at::native::FillFunctor<int>)",0,1,0,0.017467,0.0040939999999999995,3125,0.00131
107,"void cudnn::cnn::wgrad_alg0_engine<__half, int=128, int=6, int=7, int=3, int=3, int=5, bool=0, int=512>(int, int, int, __half const *, int, cudnn::cnn::wgrad_alg0_engine<__half, int=128, int=6, int=7, int=3, int=3, int=5, bool=0, int=512>*, __half const , kernel_grad_params, __int64, int, float, int, int, int, int)",0,4,0,0.016526,0.0038729999999999997,9,0.43036199999999997
72,"void explicit_convolve_sgemm<__half, int, int=128, int=6, int=7, int=3, int=3, int=5, int=0, bool=0>(int, int, int, __half const *, int, __half const *, int, __half*, kernel_conv_params, __int64, int, __int64, int, float, float, int, __half const *, __half const *)",0,5,0,0.016413999999999998,0.0038469999999999997,7,0.54957
5,"void explicit_convolve_sgemm<__half, int, int=128, int=5, int=5, int=3, int=3, int=3, int=0, bool=0>(int, int, int, __half const *, int, __half const *, int, __half*, kernel_conv_params, __int64, int, __int64, int, float, float, int, __half const *, __half const *)",0,5,0,0.015484,0.003629,14,0.25922399999999995
46,"void cudnn::detail::dgrad_engine<__half, int=512, int=6, int=5, int=3, int=3, int=3, bool=0>(int, int, int, __half const *, int, __half const , int, cudnn::detail::dgrad_engine<__half, int=512, int=6, int=5, int=3, int=3, int=3, bool=0>*, kernel_grad_params, __int64, int, __int64, int, float, int, int, int)",0,4,0,0.015113,0.003542,6,0.590376
41,volta_cgemm_64x32_tn,0,9,0,0.011956,0.002802,8,0.350279
101,"void cudnn::cnn::wgrad_alg0_engine<__half, int=128, int=5, int=5, int=3, int=3, int=3, bool=0, int=512>(int, int, int, __half const *, int, cudnn::cnn::wgrad_alg0_engine<__half, int=128, int=5, int=5, int=3, int=3, int=3, bool=0, int=512>*, __half const , kernel_grad_params, __int64, int, float, int, int, int, int)",0,3,0,0.011184999999999999,0.0026219999999999998,19,0.137974
22,"void fft2d_r2c_16x16<__half>(float2*, __half const *, int, int, int, int, int, int, int, int)",0,3,0,0.008925,0.002092,16,0.13073800000000002
99,"void cudnn::cnn::im2col4d_kernel<__half, long>(cudnn::cnn::im2col4d_params, cudnnConvolutionStruct, cudnnTensor4dStruct, __half const *, cudnnTensor4dStruct*)",0,2,0,0.007866,0.0018440000000000002,22,0.08380499999999999
37,"void fft2d_c2r_32x32<__half, bool=0, bool=0, unsigned int=1, bool=0, bool=0>(__half*, float2 const *, int, int, int, int, int, int, int, int, int, float, float, cudnn::reduced_divisor, bool, __half*, __half*, int2, int, int)",0,4,0,0.006605,0.0015480000000000001,16,0.09675399999999999
36,"void fft2d_c2r_32x32<__half, bool=1, bool=0, unsigned int=0, bool=0, bool=0>(__half*, float2 const *, int, int, int, int, int, int, int, int, int, float, float, cudnn::reduced_divisor, bool, __half*, __half*, int2, int, int)",0,4,0,0.006479,0.001519,12,0.126545
95,"void flip_filter<__half, __half>(__half*, __half const *, int, int, int, int)",0,1,0,0.005632,0.00132,15,0.088001
44,"void fft2d_r2c_32x32<__half, bool=0, unsigned int=0, bool=0>(float2*, __half const *, int, int, int, int, int, int, int, int, int, cudnn::reduced_divisor, bool, int2, int, int)",0,3,0,0.0054009999999999996,0.001266,29,0.043651
112,"void transpose_readWrite_alignment_kernel<float2, float2, int=1, bool=0, int=6, int=4, int=4>(cublasTransposeParams<float2>, float2 const *, float2*, float2 const *)",0,1,0,0.005187,0.001216,9,0.13509100000000002
111,"void cudnn::detail::dgrad_alg1_engine<__half, int=512, int=6, int=5, int=3, int=3, int=3, bool=0, bool=1>(int, int, int, __half const *, int, __half const , int, cudnn::detail::dgrad_alg1_engine<__half, int=512, int=6, int=5, int=3, int=3, int=3, bool=0, bool=1>*, kernel_grad_params, __int64, int, float, int)",0,3,0,0.0050810000000000004,0.001191,2,0.595416
126,void cutlass::Kernel<cutlass_70_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_tn_align2>(cutlass_70_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_tn_align2Params),0,0,0,0.0046229999999999995,0.001083,78,0.013890000000000001
43,"void fft2d_r2c_64x64<__half, bool=1>(float2*, __half const *, int, int, int, int, int, int, int, int)",0,4,0,0.004575,0.001072,14,0.076586
9,"void pointwise_mult_and_sum_complex<float2, int=8, int=4>(float2*, float2*, float2*, int, int, int, int, int, float2)",0,5,0,0.003997,0.0009369999999999999,4,0.234181
40,"void fft2d_c2r_64x64<__half, bool=0, bool=1>(__half*, float2*, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half*)",0,4,0,0.003871,0.000907,7,0.129616
15,"void fft2d_c2r_32x32<__half, bool=0, bool=0, unsigned int=0, bool=0, bool=0>(__half*, float2 const *, int, int, int, int, int, int, int, int, int, float, float, cudnn::reduced_divisor, bool, __half*, __half*, int2, int, int)",0,2,0,0.003804,0.000892,20,0.04458
18,"void fft2d_c2r_16x16<__half, bool=0>(__half*, float2*, int, int, int, int, int, int, int, int, int, int, float, float, int, __half*, __half*)",0,3,0,0.003786,0.0008869999999999999,8,0.11091
35,"void cudnn::cnn::wgrad_alg1_engine<__half, int=128, int=5, int=5, int=3, int=3, int=3, bool=0, bool=1>(int, int, int, __half const *, int, float*, __half const , kernel_grad_params, __int64, int, float, float, int, int, int*, kernel_grad_params, int, int)",0,2,0,0.003452,0.0008089999999999999,2,0.404555
68,"void cudnn::detail::dgrad_alg1_engine<__half, int=128, int=6, int=7, int=3, int=3, int=5, bool=0, bool=0>(int, int, int, __half const *, int, __half const , int, cudnn::detail::dgrad_alg1_engine<__half, int=128, int=6, int=7, int=3, int=3, int=5, bool=0, bool=0>*, kernel_grad_params, __int64, int, float, int)",0,7,0,0.0032530000000000002,0.000762,1,0.7623260000000001
127,"void precomputed_convolve_sgemm<__half, int=128, int=6, int=7, int=3, int=3, int=5, int=1, bool=0>(int, int, int, __half const *, int, __half*, __half const *, kernel_conv_params, __int64, int, float, float, int, __half const *, __half const *, int*)",0,0,0,0.003003,0.000704,1,0.7039270000000001
128,"void at::native::reduce_kernel<int=512, int=1, at::native::ReduceOp<c10::Half, at::native::MaxOps<c10::Half>, unsigned int, c10::Half, int=4>>(c10::Half)",0,0,0,0.002843,0.000666,79,0.008434
102,"void cudnn::detail::dgrad2d_alg1_1<__half, int=0, int=5, int=6, int=4, int=3, int=4, bool=0, bool=1>(int, int, int, __half const *, int, __half const , int, cudnn::detail::dgrad2d_alg1_1<__half, int=0, int=5, int=6, int=4, int=3, int=4, bool=0, bool=1>*, kernel_grad_params, __int64, int, __int64, int, float, int, int)",0,8,0,0.002826,0.000662,3,0.220797
106,"void cudnn::cnn::wgrad_alg1_engine<__half, int=512, int=6, int=5, int=3, int=3, int=3, bool=0, bool=0>(int, int, int, __half const *, int, float*, __half const , kernel_grad_params, __int64, int, float, float, int, int, int*, kernel_grad_params, int, int)",0,4,0,0.00237,0.0005549999999999999,1,0.55548
64,"void cudnn::cnn::wgrad_alg1_engine<__half, int=128, int=5, int=5, int=3, int=3, int=3, bool=0, bool=0>(int, int, int, __half const *, int, float*, __half const , kernel_grad_params, __int64, int, float, float, int, int, int*, kernel_grad_params, int, int)",0,2,0,0.002332,0.000547,3,0.18218399999999998
38,"void cudnn::cnn::wgrad_alg1_engine<__half, int=128, int=6, int=7, int=3, int=3, int=5, bool=0, bool=0>(int, int, int, __half const *, int, float*, __half const , kernel_grad_params, __int64, int, float, float, int, int, int*, kernel_grad_params, int, int)",0,7,0,0.002259,0.00053,3,0.17651
25,"void cudnn::ops::convertTensor_kernel<__half, float, float, int=0>(float, __half const *, cudnn::ops::convertTensor_kernel<__half, float, float, int=0>, float*, unsigned long)",0,2,0,0.0022,0.000516,38,0.013571000000000001
129,volta_fp16_scudnn_fp16_128x64_relu_medium_nn_v1,0,0,0,0.00216,0.0005059999999999999,1,0.50617
130,_ZN2at6native13reduce_kernelILi512ELi1ENS0_8ReduceOpIlNS0_14func_wrapper_tIlZNS0_11sum_functorIlllEclERNS_14TensorIteratorEEUlllE_EEjlLi4EEEEEvT1_,0,0,0,0.001918,0.00045,79,0.005691
69,"void cudnn::ops::convertTensor_kernel<float, __half, float, int=0>(float, float const *, cudnn::ops::convertTensor_kernel<float, __half, float, int=0>, __half*, unsigned long)",0,2,0,0.0018690000000000002,0.00043799999999999997,27,0.016222999999999998
31,volta_fp16_scudnn_fp16_128x128_relu_small_nn_v1,0,9,0,0.001702,0.000399,2,0.199405
131,_ZN2at6native27unrolled_elementwise_kernelIZZZNS0_21copy_device_to_deviceERNS_14TensorIteratorEbENKUlvE0_clEvENKUlvE10_clEvEUllE_NS_6detail5ArrayIPcLi2EEE23TrivialOffsetCalculatorILi1EjESC_NS0_6memory12LoadWithCastILi1EEENSD_13StoreWithCastEEEviT_T0_T1_T2_T3_T4_,0,0,0,0.001572,0.00036899999999999997,79,0.004665
67,"void cudnn::cnn::wgrad_alg1_engine<__half, int=128, int=6, int=8, int=3, int=3, int=5, bool=0, bool=0>(int, int, int, __half const *, int, float*, __half const , kernel_grad_params, __int64, int, float, float, int, int, int*, kernel_grad_params, int, int)",0,7,0,0.001483,0.000348,2,0.173821
27,"void cudnn::detail::dgrad2d_alg1_1<__half, int=0, int=6, int=7, int=5, int=4, int=5, bool=0, bool=1>(int, int, int, __half const *, int, __half const , int, cudnn::detail::dgrad2d_alg1_1<__half, int=0, int=6, int=7, int=5, int=4, int=5, bool=0, bool=1>*, kernel_grad_params, __int64, int, __int64, int, float, int, int)",0,7,0,0.00145,0.00033999999999999997,2,0.169918
34,volta_scudnn_128x32_stridedB_splitK_xregs_large_nn_v1,0,5,0,0.001102,0.000258,1,0.258397
81,volta_fp16_scudnn_fp16_128x128_stridedB_medium_nn_v1,0,0,0,0.001098,0.00025699999999999996,1,0.257276
132,[CUDA memcpy DtoD],0,0,0,0.000843,0.000198,62,0.0031850000000000003
59,volta_scudnn_128x32_stridedB_splitK_small_nn_v1,0,6,0,0.000778,0.000182,2,0.09113500000000001
45,"void cudnn::winograd::generateWinogradTilesKernel<int=0, float, float>(cudnn::winograd::GenerateWinogradTilesParams<float, float>)",0,1,0,0.0007639999999999999,0.00017900000000000001,14,0.012786
42,volta_scudnn_128x64_stridedB_splitK_small_nn_v1,0,6,0,0.000727,0.00016999999999999999,2,0.085199
77,"void precomputed_convolve_sgemm<__half, int=1024, int=5, int=5, int=4, int=3, int=3, int=1, bool=0>(int, int, int, __half const *, int, __half*, __half const *, kernel_conv_params, __int64, int, float, float, int, __half const *, __half const *, int*)",0,6,0,0.000692,0.000162,1,0.162302
133,"void at::native::vectorized_elementwise_kernel<int=4, at::native::CompareEqFunctor<long>, at::detail::Array<char*, int=3>>(int, long, at::native::CompareEqFunctor<long>)",0,0,0,0.000674,0.00015800000000000002,79,0.001998
23,_Z25multi_tensor_apply_kernelI18TensorListMetadataILi2EE12ScaleFunctorIffEJfEEviPViT_T0_DpT1_,0,1,0,0.000566,0.000133,1,0.132639
79,"void explicit_convolve_sgemm<__half, int, int=1024, int=5, int=5, int=3, int=3, int=3, int=0, bool=0>(int, int, int, __half const *, int, __half const *, int, __half*, kernel_conv_params, __int64, int, __int64, int, float, float, int, __half const *, __half const *)",0,4,0,0.000559,0.000131,1,0.13103800000000002
89,volta_fp16_scudnn_fp16_128x128_relu_medium_nn_v1,0,6,0,0.000539,0.000126,3,0.042111
58,"void fft2d_r2c_32x32<__half, bool=0, unsigned int=5, bool=0>(float2*, __half const *, int, int, int, int, int, int, int, int, int, cudnn::reduced_divisor, bool, int2, int, int)",0,4,0,0.00031299999999999996,7.3e-05,1,0.073439
61,"void fft2d_r2c_32x32<__half, bool=1, unsigned int=0, bool=0>(float2*, __half const *, int, int, int, int, int, int, int, int, int, cudnn::reduced_divisor, bool, int2, int, int)",0,5,0,0.000226,5.3e-05,1,0.052926999999999995
47,volta_s884cudnn_fp16_256x128_ldg8_wgrad_idx_exp_interior_nhwc_nt_v1,5,1,0,0.00018,4.2e-05,1,0.042272000000000004
134,volta_fp16_s884cudnn_fp16_256x128_ldg8_relu_f2f_exp_interior_nhwc2nchw_tn_v1,0,0,0,0.000153,3.6e-05,1,0.035838999999999996
56,"void nchwToFoldedNchwKernel<__half, __half, float, bool=1, cudnnKernelDataType_t=0>(int, int, int, int, __half const *, __half*, int, int, int, int, int, int, int, int, int, int, int, float, float, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divisor)",0,3,0,9.7e-05,2.3e-05,2,0.011406999999999999
33,"void nchwAddPaddingKernel<__half, __half, float, bool=1, cudnnKernelDataType_t=0>(int, int, int, int, int, int, int, int, __half const *, __half*, int, int, int, int, int, float, float, cudnn::reduced_divisor, cudnn::reduced_divisor, cudnn::reduced_divisor)",0,3,0,5.6000000000000006e-05,1.3000000000000001e-05,2,0.006512
104,cask_cudnn::computeWgradSplitKOffsetsKernel(cask_cudnn::ComputeSplitKOffsetsParams),0,1,0,4.2e-05,1e-05,5,0.001952
51,cask_cudnn::computeWgradBOffsetsKernel(cask_cudnn::ComputeWgradBOffsetsParams),0,1,0,4.2e-05,1e-05,5,0.001945
96,"void cudnn::cnn::kern_precompute_indices<bool=0>(int*, int, int, int, int, int, int)",0,1,0,1.7e-05,4e-06,2,0.002
